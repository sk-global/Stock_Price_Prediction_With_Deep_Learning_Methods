# -*- coding: utf-8 -*-
"""Forecast with CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wvAXUnzZRykE80KiJOcDSS_vfKWrp8zp
"""

from google.colab import files
 
data_file = files.upload()

import pandas as pd
import numpy as np


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, concatenate
from tensorflow.keras.layers import LSTM, SimpleRNN, Conv1D, Bidirectional
from tensorflow.keras.models import Sequential, clone_model, Model
from tensorflow.keras.optimizers import RMSprop, Adam
from tensorflow.keras import regularizers, metrics, callbacks
from tensorflow.keras.callbacks import TensorBoard
from keras import layers
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
import time
import datetime, os

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

data_df = pd.read_csv('data.csv',index_col = 0)

data_df.set_index(pd.DatetimeIndex(data_df.index), inplace=True)

data_df = data_df.drop('returns', axis=1)
# shift mid price by 1
shifted_df= data_df.shift()
# merge mid and Shifter mid values
concat_df = [data_df, shifted_df[['mid']]]
data = pd.concat(concat_df,axis=1)
# Replace NaNs with 0
data.fillna(0, inplace=True)
data.columns = [*data.columns[:-1], 'y']
data.head()

# unselecting redundant columns with high corelation among each other, e.g. (b1 & b3), (b6 & b9 & b15), (b7 & b15) so taking out extra based on Question 1
selected_features = [x for x in data.columns if x not in ['b3','b6','b7','b9']]
selected_data = data[selected_features]

# Variables
n_points, n_features = selected_data.shape
print(n_points, n_features)
n_timestamps = 1
batch_size = 32
epochs=2
val_perct = 0.1
test_perct = 0.1
seed = 42

def get_dataset(data, val_perct = 0.1, test_perct = 0.1, batch_size = 32, n_timestamps = 1):

    num_val_samples = int(data.shape[0]*val_perct)
    num_test_samples = int(data.shape[0]*test_perct)
    num_train_samples = data.shape[0] - num_val_samples - num_test_samples
    train_data = data.iloc[0 : num_train_samples]
    val_data = data.iloc[num_train_samples: num_train_samples+num_val_samples]
    test_data = data.iloc[num_train_samples+num_val_samples:]

    # Normalize the data
    scaler = StandardScaler()
    scaled_train_data = scaler.fit_transform(train_data)
    scaled_val_data = scaler.transform(val_data)
    scaled_test_data = scaler.transform(test_data)

    # split features and target
    x_train = np.expand_dims(scaled_train_data[:,:-1].astype("float32"), n_timestamps)
    y_train = scaled_train_data[:,-1].astype("float32")
    cnn_x_train = x_train[:,:,:2]

    x_val = np.expand_dims(scaled_val_data[:,:-1].astype("float32"), n_timestamps)
    y_val = scaled_val_data[:,-1].astype("float32")
    cnn_x_val = x_val[:,:,:2]

    x_test = np.expand_dims(scaled_test_data[:,:-1].astype("float32"), n_timestamps)
    y_test = scaled_test_data[:,-1].astype("float32")
    cnn_x_test = x_test[:,:,:2]

    return (
        tf.data.Dataset.from_tensor_slices(((cnn_x_train, x_train), y_train)).batch(batch_size),
        tf.data.Dataset.from_tensor_slices(((cnn_x_val, x_val), y_val)).batch(batch_size),
        tf.data.Dataset.from_tensor_slices(((cnn_x_test, x_test), y_test)).batch(batch_size)
    )

train_dataset, val_dataset, test_dataset = get_dataset(selected_data, val_perct = val_perct, test_perct = test_perct, batch_size = batch_size, n_timestamps = n_timestamps)

# CNN Model
cnn_input = Input(shape=(n_timestamps,2))
conv1 = keras.layers.Conv1D(filters=512, kernel_size=3, padding="same", activation='relu')(cnn_input)
conv1 = keras.layers.BatchNormalization()(conv1)
conv1 = keras.layers.Conv1D(filters=256, kernel_size=3, padding="same", activation='relu')(conv1)
conv1 = keras.layers.BatchNormalization()(conv1)
conv_flatten = Flatten()(conv1)
conv_output = Dense(1, activation="linear")(conv_flatten)

# Earlier Model
model1_input = Input(shape=(n_timestamps,n_features-1))
model1_1 = SimpleRNN(512, activation="relu")(model1_input)
model1_2 = Dense(512, activation="relu")(model1_1)
model1_output = Dense(1, activation = "linear")(model1_2)

#Merging CNN and Earlier Model
cnn_model1 = concatenate([conv_output,model1_output])

name = "cnn_with_model1"
logdir = os.path.join("logs", name, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

#Final Layer
output_layer = Dense(1, activation = "linear")(cnn_model1)

#Model Definition 
merged_model = Model(inputs=[(cnn_input,model1_input)],outputs=[output_layer])

!rm -rf ./logs/

path_checkpoint = "model2_checkpoint.h5"
modelckpt_callback = callbacks.ModelCheckpoint(monitor="val_loss", filepath=path_checkpoint, verbose=1, save_weights_only=False, save_best_only=True)
earlystopping = callbacks.EarlyStopping(monitor ="val_loss", mode ="min", patience = 5)

merged_model.compile(loss='mae', optimizer='adam')
merged_model.fit(train_dataset, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=val_dataset, callbacks=[tensorboard_callback, earlystopping, modelckpt_callback])

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs