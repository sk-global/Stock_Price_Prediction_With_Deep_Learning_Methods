# -*- coding: utf-8 -*-
"""Forecast with LSTM, CNN and RNN combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VAG-bMeSHTnyMsFrdQ4cJEaQttDLpIAU
"""

from google.colab import files
 
data_file = files.upload()

import pandas as pd
import numpy as np


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, concatenate
from tensorflow.keras.layers import LSTM, SimpleRNN, Conv1D, Bidirectional
from tensorflow.keras.models import Sequential, clone_model, Model
from tensorflow.keras.optimizers import RMSprop, Adam
from tensorflow.keras import regularizers, metrics, callbacks
from tensorflow.keras.callbacks import TensorBoard
from keras import layers
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt
import time
import datetime, os

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

data_df = pd.read_csv('data.csv',index_col = 0)

data_df.set_index(pd.DatetimeIndex(data_df.index), inplace=True)

# Below is used to shift data by 1 and take mid as output to predict, discarding the returns column as well

data_df = data_df.drop('returns', axis=1)
# shift mid price by 1
shifted_df= data_df.shift()
# merge mid and Shifter mid values
concat_df = [data_df, shifted_df[['mid']]]
data = pd.concat(concat_df,axis=1)
# Replace NaNs with 0
data.fillna(0, inplace=True)
data.columns = [*data.columns[:-1], 'y']
data.head()

# unselecting redundant columns with high corelation among each other, e.g. (b1 & b3), (b6 & b9 & b15), (b7 & b15) so taking out extra based on Answer 1 heat map
selected_features = [x for x in data.columns if x not in ['b3','b6','b7','b9']]
selected_data = data[selected_features]

# Variables
n_points, n_features = selected_data.shape
print(n_points, n_features)
n_timestamps = 1
batch_size = 32
epochs=20
val_perct = 0.1
test_perct = 0.1
seed = 42

# Create a MirroredStrategy for running parallelly on multiple devices on a single host.
strategy = tf.distribute.MirroredStrategy()
print("Number of devices: {}".format(strategy.num_replicas_in_sync))

batch_size = batch_size * strategy.num_replicas_in_sync

def get_dataset(data, val_perct = 0.1, test_perct = 0.1, batch_size = 32, n_timestamps = 1):

    num_val_samples = int(data.shape[0]*val_perct)
    num_test_samples = int(data.shape[0]*test_perct)
    num_train_samples = data.shape[0] - num_val_samples - num_test_samples
    train_data = data.iloc[0 : num_train_samples]
    val_data = data.iloc[num_train_samples: num_train_samples+num_val_samples]
    test_data = data.iloc[num_train_samples+num_val_samples:]

    # Normalize the data
    scaler = StandardScaler()
    scaled_train_data = scaler.fit_transform(train_data)
    scaled_val_data = scaler.transform(val_data)
    scaled_test_data = scaler.transform(test_data)

    # split features and target
    x_train = np.expand_dims(scaled_train_data[:,:-1].astype("float32"), n_timestamps)
    y_train = scaled_train_data[:,-1].astype("float32")
    cnn_x_train = x_train[:,:,:2]
    lstm_x_train = x_train[:,:,-1:]

    x_val = np.expand_dims(scaled_val_data[:,:-1].astype("float32"), n_timestamps)
    y_val = scaled_val_data[:,-1].astype("float32")
    cnn_x_val = x_val[:,:,:2]
    lstm_x_val = x_val[:,:,-1:]

    x_test = np.expand_dims(scaled_test_data[:,:-1].astype("float32"), n_timestamps)
    y_test = scaled_test_data[:,-1].astype("float32")
    cnn_x_test = x_test[:,:,:2]
    lstm_x_test = x_test[:,:,-1:]

    return (
        tf.data.Dataset.from_tensor_slices((((lstm_x_train, cnn_x_train), x_train), y_train)).batch(batch_size),
        tf.data.Dataset.from_tensor_slices((((lstm_x_val, cnn_x_val), x_val), y_val)).batch(batch_size),
        tf.data.Dataset.from_tensor_slices((((lstm_x_test, cnn_x_test), x_test), y_test)).batch(batch_size)
    )

train_dataset, val_dataset, test_dataset = get_dataset(selected_data, val_perct = val_perct, test_perct = test_perct, batch_size = batch_size, n_timestamps = n_timestamps)

def get_compiled_model(optim_type, num_unit):
  
  # LSTM Model
  lstm_input = Input(shape=(n_timestamps,1))
  lstm1 = Bidirectional(LSTM(num_unit))(lstm_input)
  lstm1 = Dense(num_unit, activation="relu")(lstm1)
  lstm_output = Dense(1, activation = "linear")(lstm1)

  # CNN Model
  cnn_input = Input(shape=(n_timestamps,2))
  conv1 = keras.layers.Conv1D(filters=num_unit, kernel_size=3, padding="same", activation='relu')(cnn_input)
  conv1 = keras.layers.BatchNormalization()(conv1)
  conv1 = keras.layers.Conv1D(filters=int(num_unit // 2), kernel_size=3, padding="same", activation='relu')(conv1)
  conv1 = keras.layers.BatchNormalization()(conv1)
  conv_flatten = Flatten()(conv1)
  conv_output = Dense(1, activation="linear")(conv_flatten)

  # Earlier Model from Answer 1
  model1_input = Input(shape=(n_timestamps,n_features-1))
  model1 = SimpleRNN(num_unit, activation="relu")(model1_input)
  model1 = Dense(num_unit, activation="relu")(model1)
  model1_output = Dense(1, activation = "linear")(model1)

  #Merging LSTM, CNN and Earlier Model
  lstm_cnn_model1 = concatenate([lstm_output, conv_output, model1_output])

  #Final Layer
  output_layer = Dense(1, activation = "linear")(lstm_cnn_model1)

  #Merged Model Definition 
  merged_model = Model(inputs=[(lstm_input,cnn_input,model1_input)],outputs=[output_layer])

  merged_model.compile(loss='mae', optimizer=optim_type)

  return merged_model

# Clear earlier Tensorboard logs
!rm -rf ./logs/

tf.random.set_seed(seed)

optim_types = ['adam', 'rmsprop']
num_units = [256, 512]

path_checkpoint = "model4_checkpoint.h5"
modelckpt_callback = callbacks.ModelCheckpoint(monitor="val_loss", filepath=path_checkpoint, verbose=1, save_weights_only=False, save_best_only=True)
earlystopping = callbacks.EarlyStopping(monitor ="val_loss", mode ="min", patience = 5)

for optim_type in optim_types:
      for num_unit in num_units:
        name = "MultiDevice_lstm_with_cnn_with_model1-{}-{}-units-".format(optim_type, num_unit)
        logdir = os.path.join("logs", name, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
        tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
        
        with strategy.scope():
          model = get_compiled_model(optim_type, num_unit)
        
        model.fit(train_dataset, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=val_dataset, callbacks=[tensorboard_callback, earlystopping, modelckpt_callback])

model = keras.models.load_model(path_checkpoint)
model.evaluate(test_dataset)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

